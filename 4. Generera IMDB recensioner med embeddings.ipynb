{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as its\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.preprocessing.sequence as pps\n",
    "import keras.layers.wrappers as wrappers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# We get rid of most possible characters:\n",
    "chars_to_keep = string.ascii_lowercase  + string.digits + ' ' + ',' + '.' + '?' + '!' + '(' + ')' + '/' + ':' + ';' + '\\\"'\n",
    "\n",
    "char_to_idx = { c : i for i, c in enumerate(chars_to_keep)}\n",
    "idx_to_char = {i : c for i, c in enumerate(chars_to_keep)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def string_to_array(s):\n",
    "    # map maybe_char() to input to remove uncomon characters:\n",
    "    def maybe_char(c):\n",
    "        if c in chars_to_keep:\n",
    "            return c\n",
    "        return \"\"\n",
    "    # Return a numpy array\n",
    "    return np.array([char_to_idx[c] for c in \"\".join(map(maybe_char, s.lower()))], np.int8)\n",
    "\n",
    "\n",
    "\n",
    "def get_data(folder='aclImdb', files_to_use=1.0, max_len=800, train_data=True):\n",
    "    # Takes the first files_to_use files from both folders (max 25000 of each) and pads with zeros to max_len\n",
    "    \n",
    "    if train_data:\n",
    "        folder += '/train'\n",
    "    else:\n",
    "        folder += '/test'\n",
    "        \n",
    "    X = []\n",
    "    X_lengths = []\n",
    "    lengths = []\n",
    "    sub_folders = ['pos', 'neg']\n",
    "    for sub_folder in sub_folders:\n",
    "        path = folder + '/' + sub_folder + \"/\"\n",
    "        files = os.listdir(path)\n",
    "        \n",
    "        # Only take the fraction files_to_use \n",
    "        files = files[:int(len(files)*files_to_use)]\n",
    "        lengths.append(len(files))\n",
    "\n",
    "        for fn in files:\n",
    "\n",
    "            try:\n",
    "                f = file(path + fn, 'r')\n",
    "                ss = f.read()\n",
    "                f.close()\n",
    "            except Exception as e:\n",
    "                print \"Could not open file %s, error: %s %s %s\"% (fn, type(e), e.__str__(), sys.exc_info()[0])\n",
    "            \n",
    "            X.append(string_to_array(ss))\n",
    "            X_lengths.append(len(ss))\n",
    "\n",
    "    # Add one and so that we pad with zeros (zeros will be masked). First only positive, then negative.\n",
    "    X = keras.preprocessing.sequence.pad_sequences(X, max_len, dtype='int32', value=-1) + 1\n",
    "    y = np.concatenate((np.ones(lengths[0]), np.zeros(lengths[1])))\n",
    "    \n",
    "    np.random.seed(7)\n",
    "    np.random.shuffle(X)\n",
    "    \n",
    "    np.random.seed(7)\n",
    "    np.random.shuffle(y)\n",
    "    \n",
    "    return X, y[:, np.newaxis], X_lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUkcZAJLUUQaAJHWUASBJHfX/AeQ0t3uWv0IdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108dd8710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(get_data(max_len=5000)[2], bins=100, range=(0,4000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25003, 600)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, lengths = get_data(max_len=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = keras.models.Sequential()\n",
    "\n",
    "one_hot_embedding_lookup = np.concatenate((np.zeros((len(chars_to_keep), 1)), np.eye(len(chars_to_keep))), axis=1).T\n",
    "\n",
    "m.add( keras.layers.Embedding(len(chars_to_keep) + 1, len(chars_to_keep), mask_zero=True, trainable=False, weights=[one_hot_embedding_lookup]) )\n",
    "#m.add( keras.layers.LSTM(128, return_sequences=True))\n",
    "#m.add( keras.layers.Dropout(0.4))\n",
    "m.add( keras.layers.LSTM(128))\n",
    "#m.add( keras.layers.Dropout(0.4))\n",
    "m.add( keras.layers.Dense(len(chars_to_keep) + 1, activation='softmax') )\n",
    "\n",
    "m.compile('rmsprop', loss='sparse_categorical_crossentropy' )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2289: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 42s - loss: 3.1477 - val_loss: 3.0318\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 38s - loss: 2.9708 - val_loss: 2.8254\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 46s - loss: 2.7537 - val_loss: 2.6356\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 49s - loss: 2.6098 - val_loss: 2.4978\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 48s - loss: 2.5032 - val_loss: 2.3940\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 43s - loss: 2.4349 - val_loss: 2.3739\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 43s - loss: 2.3574 - val_loss: 2.2003\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 44s - loss: 2.3068 - val_loss: 2.2502\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 44s - loss: 2.2472 - val_loss: 2.1762\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 43s - loss: 2.2191 - val_loss: 2.1343\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 46s - loss: 2.1591 - val_loss: 2.0696\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 44s - loss: 2.1192 - val_loss: 2.0434\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 44s - loss: 2.0858 - val_loss: 2.0302\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 46s - loss: 2.0378 - val_loss: 1.9780\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 50s - loss: 2.0257 - val_loss: 1.9555\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 43s - loss: 1.9750 - val_loss: 1.8644\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 50s - loss: 1.9494 - val_loss: 1.9596\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 56s - loss: 1.9387 - val_loss: 1.8308\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 42s - loss: 1.9072 - val_loss: 1.8390\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 41s - loss: 1.8945 - val_loss: 1.8115\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 40s - loss: 1.8966 - val_loss: 1.7964\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 41s - loss: 1.8629 - val_loss: 1.7575\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 42s - loss: 1.8802 - val_loss: 1.7817\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 46s - loss: 1.8357 - val_loss: 1.7878\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 41s - loss: 1.8498 - val_loss: 1.7653\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 43s - loss: 1.8156 - val_loss: 1.7604\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 40s - loss: 1.8081 - val_loss: 1.6979\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 38s - loss: 1.7805 - val_loss: 1.7199\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 38s - loss: 1.7929 - val_loss: 1.7325\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 3643s - loss: 1.7902 - val_loss: 1.6937\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 43s - loss: 1.7876 - val_loss: 1.7000\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 43s - loss: 1.7788 - val_loss: 1.6830\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 41s - loss: 1.7756 - val_loss: 1.6926\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 40s - loss: 1.7606 - val_loss: 1.6117\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 45s - loss: 1.7502 - val_loss: 1.6492\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 44s - loss: 1.7703 - val_loss: 1.6216\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 45s - loss: 1.7524 - val_loss: 1.6324\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 45s - loss: 1.7534 - val_loss: 1.6318\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 45s - loss: 1.7362 - val_loss: 1.6598\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 46s - loss: 1.7392 - val_loss: 1.6392\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 46s - loss: 1.7388 - val_loss: 1.6530\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 45s - loss: 1.7421 - val_loss: 1.6583\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 48s - loss: 1.7204 - val_loss: 1.6157\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 55s - loss: 1.7276 - val_loss: 1.6303\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 56s - loss: 1.7310 - val_loss: 1.6507\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 56s - loss: 1.7207 - val_loss: 1.6387\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 60s - loss: 1.7414 - val_loss: 1.6144\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 65s - loss: 1.7085 - val_loss: 1.6045\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 41s - loss: 1.7222 - val_loss: 1.6407\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 41s - loss: 1.7182 - val_loss: 1.6519\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "22502/22502 [==============================] - 41s - loss: 1.7110 - val_loss: 1.5679\n",
      "Train on 22502 samples, validate on 2501 samples\n",
      "Epoch 1/1\n",
      "17280/22502 [======================>.......] - ETA: 9s - loss: 1.6981"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "n_timesteps = 10\n",
    "n_epochs = 10\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    for i in range(0, X.shape[1] - n_timesteps - 1):\n",
    "        history_list.append(m.fit(X[:, i:i+n_timesteps], X[:, i+n_timesteps, np.newaxis], validation_split=0.10, shuffle=False, batch_size=batch_size, epochs=1 ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_char(m, text=\"\"):\n",
    "     return np.random.choice( ['_'] + list(chars_to_keep), p=m.predict(string_to_array(text)[np.newaxis, :])[0])\n",
    "    \n",
    "def generate_text(m, N, text=\"k\"): # start with one character so that m.predict works.\n",
    "    for i in range(N):\n",
    "        text += generate_char(m, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(m, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
